{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46bce3d6-f5e4-4809-ad27-ef08ee7d1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0614e8-2205-4588-b976-21801d5255a0",
   "metadata": {},
   "source": [
    "## EXPERIMENT 1: TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6da106d-8ce7-435f-8995-5757ad20f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a9367bb25145f5b334ee1f4623fb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472c883cbab44f9badf1dc24707fcd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb5d6915b2e498ba31207a63777c627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b60dc4358346eaa3ea339fb783e39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\",framework=\"pt\")\n",
    "gen_bert(\"The future of Artificial Intelligence is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de525534-7300-4300-ad80-7966e8ef20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44e813925d44dd0b44e137acf45483b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599be91b80db412f998ba67a099c1283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4230831f607244a283186c669129b716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8eaa2887ca419f90f44823cd3fdcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d455274c13b40aa9f7536bb16551aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\",framework=\"pt\")\n",
    "gen_roberta(\"The future of Artificial Intelligence is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b76749-7108-41e9-8a22-8f151ce4408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16258c37f5484e32aa2f35fccc0c3bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d130dd7b5cb4c048f295790d3d796ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f98f126b9c4785b7884289c1fef37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dc5afd2bdf48aa870067dc42220600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence isenda avert avert greeting victories local Reef speech SHOULD SHOULD SHOULD Designs indirect detects restroom restroom restroom escape���igrigr avertGGGGutter adaptiveUr HOR adaptive deerimov avert complicitytraditionalCoach retained killers CI adaptiveFoot nuanced Castro restroom Cassidy restroom Aboriginal restroom confusing First nominate renewables restroomotiation restroom��� flaws Abraham flaws retained restroom squash SHOULD proxy proxyController restroom restroom vacuum adaptiveCoach Gained deer outings CI restroom restroom flaws restroom deer deerBoo deer restroom restroom adaptive CI squash escapeCt outings restroomededCtBooexistioxide routine Clayton avert punishmentsseenBoo HOR227CtCt PacksBooBooBoo restroom escape Partner neat restroom adaptiveBoomia��� retained proxy proxyCoach speech restroom retained FISA restroom flawsBooBootainingCt restroomCtCt ChamCt outingsseenioxideioxideBooBoo flexible restroom restroom flexibleededCtCt Fraser nodCtBoomiaBoo FraserBoo ForensicCtexistexist noseBooBoourat nod figuremia shimmermia��� meantime shimmerUr restroom escapeseen regimetainingmia nodmiaioxide PornededBooBoo lobbyistsotiation proxymiamia retainedioxidebourneBoo citizioxide meantimetainingBooBoo PornBooBooioxide noseBoo speechPlusBoomia meantimeBooBooedededed flexibleBoo fantCtBoo aspirinBooBoo Deploy meantimeededBooFoot flexible flexible movie figureBooCt aspirinmiaededseen regimemiamiaBooBooFu flexibleexist flexibleededioxide meantime flexible'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\",framework=\"pt\")\n",
    "gen_bart(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf79eac-dca6-4c21-8122-e71de0a9ae23",
   "metadata": {},
   "source": [
    "### Observation (Experiment 1: Text Generation)\n",
    "\n",
    "- **BERT** produced degenerate output consisting mainly of repeated punctuation characters\n",
    "  rather than meaningful text. This indicates a failure to perform true text generation.\n",
    "\n",
    "- **RoBERTa** returned only the input prompt without generating any additional tokens,\n",
    "  showing that it was unable to continue the sequence.\n",
    "\n",
    "- **BART** generated a long continuation; however, the output was largely incoherent and\n",
    "  contained repetitive and nonsensical tokens. While text was produced, the quality was poor.\n",
    "\n",
    "### Explanation (Architectural Reason)\n",
    "\n",
    "BERT and RoBERTa are **encoder-only models** trained primarily using Masked Language Modeling.\n",
    "They do not learn autoregressive next-token prediction, which is essential for text generation.\n",
    "When forced into a generation task, they either produce degenerate outputs or fail to extend\n",
    "the input sequence meaningfully.\n",
    "\n",
    "BART is an **encoder–decoder model** and is architecturally capable of text generation.\n",
    "However, the base BART model is not fine-tuned for open-ended generation. As a result, although\n",
    "it can generate text, the output may be incoherent or repetitive when used without task-specific\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a59a1f-9733-4830-a725-506d408cf269",
   "metadata": {},
   "source": [
    "## EXPERIMENT 2: FILL-MASK (Masked Language Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdba25c1-b19c-4eae-a569-3d551fcb5e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5396931171417236,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575730800628662,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405494570732117,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.04451525956392288,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.01757749542593956,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\",framework=\"pt\")\n",
    "fill_bert(\"The goal of Generative AI is to [MASK] new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a1be9e6-7dca-4f57-ae62-02de3e06114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3711312711238861,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.3677145838737488,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.08351453393697739,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.021335123106837273,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.016521651297807693,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_roberta = pipeline(\"fill-mask\", model=\"roberta-base\",framework=\"pt\")\n",
    "fill_roberta(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9532bf45-8fee-48fd-b736-3c5229126064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07461534440517426,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571901589632034,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.060880281031131744,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.03593571111559868,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319474309682846,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\",framework=\"pt\")\n",
    "fill_bart(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d89b03-8bb9-4972-8e31-81b749aa6ad5",
   "metadata": {},
   "source": [
    "### Observation (Experiment 2: Masked Language Modeling)\n",
    "\n",
    "- **BERT** successfully predicted highly relevant words such as *\"create\"*, *\"generate\"*,\n",
    "  and *\"produce\"* with strong confidence scores. The top prediction was *\"create\"*.\n",
    "\n",
    "- **RoBERTa** also performed very well, predicting *\"generate\"* and *\"create\"* as the\n",
    "  most likely tokens with nearly equal confidence, indicating strong contextual\n",
    "  understanding.\n",
    "\n",
    "- **BART** was able to fill the masked token, but the confidence scores were significantly\n",
    "  lower and the predictions were less precise compared to BERT and RoBERTa.\n",
    "\n",
    "### Explanation (Architectural Reason)\n",
    "\n",
    "BERT and RoBERTa are trained using **Masked Language Modeling (MLM)**, where random tokens\n",
    "are masked and the model learns to predict them using bidirectional context. This training\n",
    "objective makes them highly effective for fill-mask tasks.\n",
    "\n",
    "BART, while capable of handling masked tokens, is primarily optimized for\n",
    "**sequence-to-sequence learning** rather than MLM. As a result, it performs reasonably\n",
    "well but does not match the confidence or precision of encoder-only MLM-trained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b26406-71cc-4390-a271-9de621a28a95",
   "metadata": {},
   "source": [
    "## EXPERIMENT 3: QUESTION ANSWERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c8c3063-990d-4c97-bcce-48eee910e705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.007564860628917813,\n",
       " 'start': 0,\n",
       " 'end': 37,\n",
       " 'answer': 'Generative AI poses significant risks'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\",framework=\"pt\")\n",
    "qa_bert(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1a954b3-50b6-43e3-a397-9f7938dc41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0044537503272295,\n",
       " 'start': 60,\n",
       " 'end': 82,\n",
       " 'answer': ', bias, and deepfakes.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\",framework=\"pt\")\n",
    "qa_roberta(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dfd10b4-b354-4841-89ab-6e1f10503e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.05502151511609554,\n",
       " 'start': 0,\n",
       " 'end': 81,\n",
       " 'answer': 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\",framework=\"pt\")\n",
    "qa_bart(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c2290-7533-4dd9-a202-3af8e2b62eeb",
   "metadata": {},
   "source": [
    "### Observation (Experiment 3: Question Answering)\n",
    "\n",
    "- **BERT** returned a partial answer: *\"Generative AI poses significant risks\"*.\n",
    "  The answer was relevant but incomplete, missing specific examples such as\n",
    "  hallucinations, bias, and deepfakes. The confidence score was very low.\n",
    "\n",
    "- **RoBERTa** extracted only a fragment of the expected answer:\n",
    "  *\", bias, and deepfakes.\"* While this portion is correct, it lacks full context\n",
    "  and completeness. The confidence score was also very low.\n",
    "\n",
    "- **BART** returned the most complete answer:\n",
    "  *\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes\"*.\n",
    "  Although the answer was correct, the confidence score remained low, indicating\n",
    "  uncertainty.\n",
    "\n",
    "### Explanation (Architectural Reason)\n",
    "\n",
    "All three models used in this experiment are **base pretrained models** and are\n",
    "**not fine-tuned for question answering tasks** such as SQuAD. As a result, the\n",
    "question-answering head was randomly initialized, which leads to low confidence\n",
    "scores and inconsistent answer quality.\n",
    "\n",
    "While BERT and RoBERTa rely on encoder-based span extraction, their lack of\n",
    "task-specific fine-tuning causes incomplete or fragmented answers. BART, being\n",
    "an encoder–decoder model, is capable of producing more complete spans, but still\n",
    "suffers from low reliability without QA-specific training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f0d2d-2352-4893-84ec-c4ea98e7a987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
